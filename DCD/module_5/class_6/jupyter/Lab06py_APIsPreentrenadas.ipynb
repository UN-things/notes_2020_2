{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab06 - APIs Preentrnados en AWS  \n",
    "\n",
    "Ya hemos visto diferentes maneras de implementar nuestro modelo de IA (Bien sea ML o DL) para poder ser usado en un ambiente productivo.  También vimos como, algunos proveedores de servicios en la Nube ofrecen modelos preentrenadospara para casos de uso comunes que podemos usar inmediatamente sin necsidad de entrenarlos en nuestro propio dataset.  \n",
    "\n",
    "![AWS](https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcT80l73TdYIEWsMWflGX19pkqw5VbE61MSS5g&usqp=CAU)\n",
    "\n",
    "**Ventajas**  \n",
    "- No hay que entrenar un modelo desde ceros\n",
    "- No es necesario conocer todos los algoritmos e hiperparámetros involucrados\n",
    "- Solo pago por las veces que invoco el modelo  \n",
    "\n",
    "**Desventajas**  \n",
    "- Funciona solo para casos de uso específicos, muy generales\n",
    "- No se tiene control de el algorítmo o hiperparámetros utilizados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "import datetime\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "\n",
    "from botocore.client import Config\n",
    "%pylab inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lo primero que debemos hacer para utilizar los servicios en la nube de AWS es crear una [cuenta](https://portal.aws.amazon.com/billing/signup#/start) y generar dentro de ella un usuario y un [API key](https://console.aws.amazon.com/iam/home?region=us-east-1#/users) que es la clave de acceso que nos dará permisos para utilizar todas las APIs de IA.  \n",
    "Para no tomar tanto tiempo en estos pasos, **para este laboratorio** vamos a usar las siguientes credenciales temporales:  \n",
    "- *key_id* y *access_key*: Llaves de acceso (API key) creadas exclusivamente para este laboratorio\n",
    "- *aws_bucket_name*: Espacio de almacenamiento (bucket) creado exclusivamente para este laboratorio\n",
    "- *directorio*: carpeta, dentro del espacio de almacenamiento para almacenar sus imágenes **por favor cambie esta variable a su nombre o su número de cédula para no confundir su espacio de trabajo con otros estudiantes**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "key_id = 'AKIA2MH4X3ODRXLMXS76'\n",
    "access_key = 'W3oUtd1BA3F0RDe1CCU6D9ry+jeYzcqhlWPBWRxd'\n",
    "aws_bucket_name = 'laboratorio-dip'\n",
    "directorio='monita'    ## CAMBIAR NOMBRE DIRECTORIO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El servicio de APIs de modelos de IA se basan también en el servicio de almacenamiento (*Bucket*) llamado *S3* en donde se espera que se encuentran las imágenes a procesar.  Para crear su propio bucket de almacenamiento puede hacerlo [aquí](https://s3.console.aws.amazon.com/s3/home), pero para este laboratorio vaos a trabajar en un bucket ya creado: *laboratorio-dip*.  \n",
    "\n",
    "La siguiente función permite subir una imagen desde si File System al bucket *laboratorio-dip* y va a ser usada cada vez que invoquemos los modelos pues estos leen directamente del bucket:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def subir_imagen_s3(nombre_archivo) :\n",
    "    # Llamado al servicio de Almacenamiento S3\n",
    "    s3 = boto3.client('s3',\n",
    "                        aws_access_key_id=key_id,\n",
    "                        aws_secret_access_key=access_key,\n",
    "                        region_name='us-east-1',\n",
    "                        config=Config(signature_version='s3v4')\n",
    "                        )\n",
    "    \n",
    "    # Usar ese servicio para subir una imagen:\n",
    "    s3.upload_file(nombre_archivo, 'laboratorio-dip', '/'+directorio+'/'+nombre_archivo)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El servicio de APIs de modelos preentrenados de IA de Amazon se llama **[Rekognition](https://aws.amazon.com/es/rekognition/?blog-cards.sort-by=item.additionalFields.createdDate&blog-cards.sort-order=desc)** y permite invocar diferentes funciones de acuerdo al modelo que queramos utilizar:  \n",
    "\n",
    "\n",
    "### 6a. API de Detección de Objetos  \n",
    "\n",
    "Amazon Rekognition cuenta ya con un modelo de detección de objetos comunes que podemos usar directamente sobre nuestras imagenes cuando no necesitamos detectar cosas muy específicas.  Para conocer más a detalle el funcionamiento, cobros y ejemplos gráficos de este servicio podemos consultar su [documentación](https://console.aws.amazon.com/rekognition/home?region=us-east-1#/label-detection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_objects(nombre_archivo) :\n",
    "    subir_imagen_s3(nombre_archivo)\n",
    "    plt.imshow(mpimg.imread(nombre_archivo))\n",
    "    plt.yticks([])\n",
    "    plt.xticks([])\n",
    "    \n",
    "    REKOGNITION_CLIENT = boto3.client(\"rekognition\", \n",
    "                                      aws_access_key_id=key_id,\n",
    "                                      aws_secret_access_key=access_key,\n",
    "                                     region_name='us-east-1')\n",
    "    \n",
    "    detect_dict = REKOGNITION_CLIENT.detect_labels(\n",
    "        Image={\n",
    "            \"S3Object\": {\n",
    "                \"Bucket\": \"laboratorio-dip\",\n",
    "                \"Name\": \"/\"+directorio+'/'+nombre_archivo\n",
    "            }\n",
    "        },\n",
    "        MaxLabels=100,\n",
    "        MinConfidence=70.0\n",
    "    )\n",
    "    return detect_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imagen_ejemplo = 'data/ImagenesLab06/IMG_0041_2.JPG'\n",
    "detections_obj = detect_objects(imagen_ejemplo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "detections_obj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = cv2.imread(imagen_ejemplo)\n",
    "img=cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "(H, W, D) = img.shape\n",
    "print('Tamaño:'+ str((H, W, D)))\n",
    "\n",
    "for label in detections_obj['Labels'] :\n",
    "    if len(label['Instances']) > 0 :\n",
    "        clase = label['Name']\n",
    "        for deteccion in label['Instances'] :\n",
    "            x1 = int(deteccion['BoundingBox']['Left']*W)\n",
    "            y1 = int(deteccion['BoundingBox']['Top']*H)\n",
    "            x2 = int((deteccion['BoundingBox']['Left']+deteccion['BoundingBox']['Width'])*W)\n",
    "            y2 = int((deteccion['BoundingBox']['Top']+deteccion['BoundingBox']['Height'])*H)\n",
    "            #cv2.rectangle(img, (x1, 60), (420, 160), (0, 0, 255), 2)\n",
    "            cv2.rectangle(img, (x1, y1), (x2, y2), (0, 0, 255), 5)\n",
    "            cv2.putText(img, clase, (x1, y1), cv2.FONT_HERSHEY_SIMPLEX, 3, (255, 255, 255), 7)\n",
    "\n",
    "plt.figure(figsize=(15,10))\n",
    "plt.imshow(img)\n",
    "plt.yticks([])\n",
    "plt.xticks([])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6b. API de Detección de Rostros  \n",
    "\n",
    "Amazon Rekognition cuenta ya con un modelo de detección de atributos en los rostros de las personas (*género, edad, felicidad, gafas, barba, etc.*) que podemos usar directamente sobre nuestras imagenes.  Para conocer más a detalle el funcionamiento, cobros y ejemplos gráficos de este servicio podemos consultar su [documentación](https://console.aws.amazon.com/rekognition/home?region=us-east-1#/face-detection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def facial_recognition(nombre_archivo) :\n",
    "    subir_imagen_s3(nombre_archivo)\n",
    "    plt.imshow(mpimg.imread(nombre_archivo))\n",
    "    plt.yticks([])\n",
    "    plt.xticks([])\n",
    "    \n",
    "    REKOGNITION_CLIENT = boto3.client(\"rekognition\", \n",
    "                                      aws_access_key_id=key_id,\n",
    "                                      aws_secret_access_key=access_key,\n",
    "                                     region_name='us-east-1')\n",
    "    \n",
    "    return REKOGNITION_CLIENT.detect_faces(\n",
    "        Image={\n",
    "            \"S3Object\": {\n",
    "                \"Bucket\": \"laboratorio-dip\",\n",
    "                \"Name\": \"/\"+ directorio +\"/\"+nombre_archivo\n",
    "            }\n",
    "        },\n",
    "        Attributes=[\"ALL\"]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imagen_ejemplo = 'data/ImagenesLab06/IMG_0041_2.JPG'\n",
    "detections_face = facial_recognition(imagen_ejemplo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "detections_face"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = cv2.imread(imagen_ejemplo)\n",
    "img=cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "(H, W, D) = img.shape\n",
    "print('Tamaño:'+ str(len(detections_face['FaceDetails'])))\n",
    "\n",
    "for label in detections_face['FaceDetails'] :\n",
    "        x1 = int(label['BoundingBox']['Left']*W)\n",
    "        y1 = int(label['BoundingBox']['Top']*H)\n",
    "        x2 = int((label['BoundingBox']['Left']+label['BoundingBox']['Width'])*W)\n",
    "        y2 = int((label['BoundingBox']['Top']+label['BoundingBox']['Height'])*H)\n",
    "        cv2.rectangle(img, (x1, y1), (x2, y2), (255, 0, 0), 5)\n",
    "        for feature in label['Landmarks'] :\n",
    "            x = int(feature['X']*W)\n",
    "            y = int(feature['Y']*H)\n",
    "            cv2.circle(img, (x,y), 10, (255, 0, 0), -1)\n",
    "        #cv2.putText(img, clase, (x1, y1), cv2.FONT_HERSHEY_SIMPLEX, 3, (255, 255, 255), 7)\n",
    "\n",
    "plt.figure(figsize=(30,20))\n",
    "plt.imshow(img)\n",
    "plt.yticks([])\n",
    "plt.xticks([])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6c. API de Comparación de Rostros  \n",
    "\n",
    "Amazon Rekognition cuenta ya con un modelo de comparación de rostros entre dos imágenes. Este modelo nos permite, sin necesdad de hacer un modelo de reconocimiento de personas particular, detectar si una persona en una imagen (*Referencia*) se encuentra en otra imagen a analizar (*Objetivo*).  Para conocer más a detalle el funcionamiento, cobros y ejemplos gráficos de este servicio podemos consultar su [documentación](https://console.aws.amazon.com/rekognition/home?region=us-east-1#/face-comparison)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# En este caso necesitamos dos imágenes, la de referencia y la objetivo que vamos a analizar\n",
    "def compare_faces(imagen_referencia, imagen_objetivo) :\n",
    "    plt.figure(figsize=(10,8))\n",
    "    subir_imagen_s3(imagen_referencia)\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.imshow(mpimg.imread(imagen_referencia))\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    \n",
    "    subir_imagen_s3(imagen_objetivo)\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.imshow(mpimg.imread(imagen_objetivo))\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    \n",
    "    REKOGNITION_CLIENT = boto3.client(\"rekognition\", \n",
    "                                      aws_access_key_id=key_id,\n",
    "                                      aws_secret_access_key=access_key,\n",
    "                                     region_name='us-east-1')\n",
    "    \n",
    "    return REKOGNITION_CLIENT.compare_faces(\n",
    "      SourceImage={\n",
    "          'S3Object': {\n",
    "              'Bucket': \"laboratorio-dip\",\n",
    "              \"Name\": \"/\"+ directorio +\"/\"+imagen_referencia\n",
    "          }\n",
    "      },\n",
    "      TargetImage={\n",
    "          'S3Object': {\n",
    "              'Bucket': \"laboratorio-dip\",\n",
    "              \"Name\": \"/\"+ directorio +\"/\"+imagen_objetivo\n",
    "          }\n",
    "      }\n",
    "  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imagen_referencia = 'data/ImagenesLab06/MonitaRB.JPG'\n",
    "imagen_ejemplo = 'data/ImagenesLab06//IMG_0051_2.JPG'\n",
    "detect_compare = compare_faces(imagen_referencia, imagen_ejemplo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "detect_compare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = cv2.imread(imagen_ejemplo)\n",
    "img=cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "(H, W, D) = img.shape\n",
    "color = (0, 255, 0) # Verde, solo marca la primera\n",
    "\n",
    "for label in detect_compare['FaceMatches'] :\n",
    "    x1 = int(label['Face']['BoundingBox']['Left']*W)\n",
    "    y1 = int(label['Face']['BoundingBox']['Top']*H)\n",
    "    x2 = int((label['Face']['BoundingBox']['Left']+label['Face']['BoundingBox']['Width'])*W)\n",
    "    y2 = int((label['Face']['BoundingBox']['Top']+label['Face']['BoundingBox']['Height'])*H)\n",
    "    cv2.rectangle(img, (x1, y1), (x2, y2), color, 5)\n",
    "\n",
    "color = (255, 0, 0)\n",
    "for label in detect_compare['UnmatchedFaces'] :\n",
    "    x1 = int(label['BoundingBox']['Left']*W)\n",
    "    y1 = int(label['BoundingBox']['Top']*H)\n",
    "    x2 = int((label['BoundingBox']['Left']+label['BoundingBox']['Width'])*W)\n",
    "    y2 = int((label['BoundingBox']['Top']+label['BoundingBox']['Height'])*H)\n",
    "    cv2.rectangle(img, (x1, y1), (x2, y2), color, 5)\n",
    "\n",
    "plt.figure(figsize=(30,20))\n",
    "plt.imshow(img)\n",
    "plt.yticks([])\n",
    "plt.xticks([])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6d. API de OCR (detección de Caracteres) \n",
    "\n",
    "Amazon Rekognition cuenta ya con un modelo de detección de Caractéres (OCR). Este modelo nos permite, sin necesidad de hacer un modelo de reconocimiento de letras o de un idioma en particuar, detectar palabras o frases en una imagen.  Para conocer más a detalle el funcionamiento, cobros y ejemplos gráficos de este servicio podemos consultar su [documentación](https://console.aws.amazon.com/rekognition/home?region=us-east-1#/text-detection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_text_image(nombre_imagen) :\n",
    "    subir_imagen_s3(nombre_imagen)\n",
    "    plt.imshow(mpimg.imread(nombre_imagen))\n",
    "    plt.yticks([])\n",
    "    plt.xticks([])\n",
    "    \n",
    "    REKOGNITION_CLIENT = boto3.client(\"rekognition\", \n",
    "                                      aws_access_key_id=key_id,\n",
    "                                      aws_secret_access_key=access_key,\n",
    "                                     region_name='us-east-1')\n",
    "    \n",
    "    return REKOGNITION_CLIENT.detect_text(\n",
    "      Image={\n",
    "          'S3Object': {\n",
    "              'Bucket': \"laboratorio-dip\",\n",
    "              \"Name\": \"/\"+ directorio +\"/\"+nombre_imagen\n",
    "          }\n",
    "      }\n",
    "  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imagen_ejemplo = 'data/ImagenesLab06/texto_imagen.JPG'\n",
    "detections_text = detect_text_image(imagen_ejemplo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "detections_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = cv2.imread(imagen_ejemplo)\n",
    "img=cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "(H, W, D) = img.shape\n",
    "color = (0, 255, 0) # Verde, solo marca la primera\n",
    "\n",
    "for label in detections_text['TextDetections'] :\n",
    "    if label['Type'] == 'WORD' :\n",
    "        clase = label['DetectedText']\n",
    "        x1 = int(label['Geometry']['BoundingBox']['Left']*W)\n",
    "        y1 = int(label['Geometry']['BoundingBox']['Top']*H)\n",
    "        x2 = int((label['Geometry']['BoundingBox']['Left']+label['Geometry']['BoundingBox']['Width'])*W)\n",
    "        y2 = int((label['Geometry']['BoundingBox']['Top']+label['Geometry']['BoundingBox']['Height'])*H)\n",
    "        cv2.rectangle(img, (x1, y1), (x2, y2), color, 5)\n",
    "        cv2.putText(img, clase, (x1, y1), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 0, 0), 5)\n",
    "\n",
    "plt.figure(figsize=(30,20))\n",
    "plt.imshow(img)\n",
    "plt.yticks([])\n",
    "plt.xticks([])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6e. API de Reconocimiento de Celebridades \n",
    "\n",
    "Amazon Rekognition cuenta ya con un modelo de detección de celebridades. Este modelo nos permite, sin necesidad de hacer un modelo de reconocimiento facial de cada persona en particular, detectar rostros de personas reconocidas públicamente.  Para conocer más a detalle el funcionamiento, cobros y ejemplos gráficos de este servicio podemos consultar su [documentación](https://console.aws.amazon.com/rekognition/home?region=us-east-1#/celebrity-detection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_celebrities(nombre_imagen) :\n",
    "    subir_imagen_s3(nombre_imagen)\n",
    "    plt.imshow(mpimg.imread(nombre_imagen))\n",
    "    plt.yticks([])\n",
    "    plt.xticks([])\n",
    "    \n",
    "    REKOGNITION_CLIENT = boto3.client(\"rekognition\", \n",
    "                                      aws_access_key_id=key_id,\n",
    "                                      aws_secret_access_key=access_key,\n",
    "                                     region_name='us-east-1')\n",
    "    \n",
    "    return REKOGNITION_CLIENT.recognize_celebrities(\n",
    "      Image={\n",
    "          'S3Object': {\n",
    "              'Bucket': \"laboratorio-dip\",\n",
    "              \"Name\": \"/\"+ directorio +\"/\"+nombre_imagen\n",
    "          }\n",
    "      }\n",
    "  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imagen_ejemplo = \"data/ImagenesLab06/selfie_famosos.JPG\"\n",
    "detection_famosos = detect_celebrities(imagen_ejemplo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "detection_famosos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = cv2.imread(imagen_ejemplo)\n",
    "img=cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "(H, W, D) = img.shape\n",
    "color = (0, 255, 0) # Verde, solo marca la primera\n",
    "\n",
    "for label in detection_famosos['CelebrityFaces'] :\n",
    "    clase = label['Name']\n",
    "    x1 = int(label['Face']['BoundingBox']['Left']*W)\n",
    "    y1 = int(label['Face']['BoundingBox']['Top']*H)\n",
    "    x2 = int((label['Face']['BoundingBox']['Left']+label['Face']['BoundingBox']['Width'])*W)\n",
    "    y2 = int((label['Face']['BoundingBox']['Top']+label['Face']['BoundingBox']['Height'])*H)\n",
    "    cv2.rectangle(img, (x1, y1), (x2, y2), color, 5)\n",
    "    cv2.putText(img, clase, (x1, y1), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 3)\n",
    "\n",
    "plt.figure(figsize=(30,20))\n",
    "plt.imshow(img)\n",
    "plt.yticks([])\n",
    "plt.xticks([])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
