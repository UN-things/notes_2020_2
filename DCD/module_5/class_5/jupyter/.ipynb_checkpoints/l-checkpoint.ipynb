{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab05 - Tipos de Redes Neuronales  \n",
    "\n",
    "En la sesión pasada creamos nuestra primera **Red Neuronal Profunda** usando capas densas (*Fully Connected*) ya que se trataba de una regresión lineal sencilla.  \n",
    "\n",
    "Ahora, vamos a crear una **Red Neuronal Convolucional** en un problema muy común de Inteligencia Artificial: Clasificación de imágenes!  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3/dist-packages/requests/__init__.py:91: RequestsDependencyWarning: urllib3 (1.26.2) or chardet (3.0.4) doesn't match a supported version!\n",
      "  RequestsDependencyWarning)\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import requests\n",
    "\n",
    "from skimage import io\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "from keras. preprocessing.image import ImageDataGenerator\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cargar el set de datos\n",
    "Dependiendo del problema que queramos solucionar, va a ser necesario recolectar una gran cantidad de imágenes para entrenar el modelo.  Existen fuentes de datos abiertas para casos de uso generales como: personas, animales, flores, automóviles, etc.  \n",
    "Por otro lado, cuándo nuestro problema es mucho más particular y no se encuentran datasets ya creados para nuestro caso, será necesario recolectar las imágenes manualmente.  La regla siempre será: *Entre más imágenes, mejor*.  \n",
    "En cualquiera de los dos casos, la estructura recomendable para organizar las imágenes es la siguiente:\n",
    "```\n",
    "Directorio Raíz\n",
    "└───train\n",
    "│   └───clase1\n",
    "│   │       imagen01.jpg\n",
    "│   │       imagen02.jpg\n",
    "│   │       ...\n",
    "│   └───clase2\n",
    "│   │       imagen01.jpg\n",
    "│   │       imagen02.jpg\n",
    "│   │       ...\n",
    "│   └───...\n",
    "│   \n",
    "└───test\n",
    "    └───clase1\n",
    "    │       imagen01.jpg\n",
    "    │       imagen02.jpg\n",
    "    │       ...\n",
    "    └───clase2\n",
    "    │       imagen01.jpg\n",
    "    │       imagen02.jpg\n",
    "    │       ...\n",
    "    └───...\n",
    "```\n",
    "\n",
    "Los directorios no *necesariemante* deben llamarse así, pero sí es recomendable tener un set de datos de entrenamiento separado del de pruebas.  Además, los **nombres** de las carpetas dentro de estos (clase1, clase2, ...), son los que indicarán las **clases/labels** de las imagenes que están adentro.\n",
    "\n",
    "Vamos a trabajar con un dataset de Logos de algunas empresas conocidas, se puede descargar de [aquí](http://flovv.github.io/Logo_detection_deep_learning/), pero para facilidad ya se encuentra descargado y  organizado en la estructura necesaria dentro de nuestra carpeta */data*:\n",
    "![Logos_sample](http://flovv.github.io/figures/post21/flickr27-sample.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El dataset consta de 675 imágenes de 27 marcas diferentes (En promedio 25 imágenes de cada clase para entrenamiento y pruebas).  En un problema real esto es un dataset pequeño, pero hoy nos servirá para aprender otros conceptos de Redes Neuronales."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5a. Mi Primera Red Neuronal Convolucional CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuración de variables\n",
    "img_width = 32\n",
    "img_height = 32\n",
    "train_samples = 498\n",
    "validation_samples = 177\n",
    "classes = ['Adidas', 'Apple', 'BMW', 'Citroen', 'Cocacola', 'DHL', 'Fedex', 'Ferrari', 'Ford', 'Google', \n",
    "           'Heineken', 'HP', 'Intel', 'McDonalds', 'Mini', 'Nbc', 'Nike', 'Pepsi', 'Porsche', 'Puma', \n",
    "           'RedBull', 'Sprite', 'Starbucks', 'Texaco', 'Unicef', 'Vodafone', 'Yahoo']\n",
    "\n",
    "train_directory = \"LogoData/train\"\n",
    "test_directory = \"LogoData/test\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuración de Hiperparámetros:\n",
    "batch_size = 8\n",
    "learning_rate = 0.0001\n",
    "epochs = 50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Augmentation\n",
    "Como ya se ha mencionado varias veces, entre más datos tengamos, mejor.  En el caso de Computer Vision: entre más imágenes, mejor.\n",
    "Cuando no tenemos suficientes datos, la técnica de Data Augmentation permite, por medio de un algoritmo, generar imágenes artificialmente a partir de las pocas imágenes que tenemos.  Esto se logra haciendo cambios pequeños a los datos: Girar un poco la imagen, cambiando su luminocidad, resolución, moviéndola un poco, etc.  Para nosotros es un cambio pequeño, pero para el modelo es una imagen completamente diferente!\n",
    "![Data Augmenation](https://miro.medium.com/max/605/0*Utma-dS47hSoQ6Zt)  \n",
    "A nivel de código, estas son TODAS las opciones que se pueden configurar para el generador de datos, tenga en cuenta que no siempre todas aplican, por ejemplo, la imagen del león mirando a la derecha o mirando a la izquierda, sigue siendo un león, pero, nuestros logos?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Estas son TODAS las opciones!\n",
    "datagen = ImageDataGenerator(\n",
    "    featurewise_center=False,\n",
    "    samplewise_center=False,\n",
    "    featurewise_std_normalization=False,\n",
    "    samplewise_std_normalization=False,\n",
    "    zca_whitening=False,\n",
    "    zca_epsilon=1e-06,\n",
    "    rotation_range=0,\n",
    "    width_shift_range=0.0,\n",
    "    height_shift_range=0.0,\n",
    "    brightness_range=None,\n",
    "    shear_range=0.0,\n",
    "    zoom_range=0.0,\n",
    "    channel_shift_range=0.0,\n",
    "    fill_mode=\"nearest\",\n",
    "    cval=0.0,\n",
    "    horizontal_flip=False,\n",
    "    vertical_flip=False,\n",
    "    rescale=None\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "datagen = ImageDataGenerator(\n",
    "    rescale=1. / 255,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 498 images belonging to 27 classes.\n",
      "Found 177 images belonging to 27 classes.\n"
     ]
    }
   ],
   "source": [
    "train_generator = datagen.flow_from_directory(\n",
    "    directory=train_directory,\n",
    "    target_size=(img_width, img_height),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical',\n",
    "    shuffle = True,\n",
    "    seed = 123\n",
    ")\n",
    "\n",
    "validation_generator = datagen.flow_from_directory(\n",
    "    directory=test_directory,\n",
    "    target_size=(img_width, img_height),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical',\n",
    "    shuffle = True,\n",
    "    seed = 123\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hemos creado dos generadores de imagenes porque cada dataset partirá de un directorio diferente, y puede ser con transformaciones diferentes.  \n",
    "Ahora sí, podemos crear el modelo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.Sequential()\n",
    "\n",
    "model.add(keras.layers.convolutional.Conv2D(filters=16, kernel_size = (3,3), \n",
    "                                            input_shape=(img_width, img_height,3)))\n",
    "model.add(keras.layers.Activation(activation='relu'))\n",
    "model.add(keras.layers.convolutional.MaxPooling2D(pool_size = (2,2)))\n",
    "\n",
    "model.add(keras.layers.convolutional.Conv2D(filters=32, kernel_size = (3,3)))\n",
    "model.add(keras.layers.Activation(activation='relu'))\n",
    "model.add(keras.layers.convolutional.MaxPooling2D(pool_size = (2,2)))\n",
    "\n",
    "model.add(keras.layers.Flatten())\n",
    "model.add(keras.layers.Dense(64))\n",
    "model.add(keras.layers.Activation(activation='relu'))\n",
    "model.add(keras.layers.Dropout(0.5))\n",
    "model.add(keras.layers.Dense(27))\n",
    "model.add(keras.layers.Activation(activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              (None, 30, 30, 16)        448       \n",
      "_________________________________________________________________\n",
      "activation (Activation)      (None, 30, 30, 16)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 15, 15, 16)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 13, 13, 32)        4640      \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 13, 13, 32)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 6, 6, 32)          0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 1152)              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 64)                73792     \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 27)                1755      \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 27)                0         \n",
      "=================================================================\n",
      "Total params: 80,635\n",
      "Trainable params: 80,635\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En el resumen que nos entrega el modelo podemos ver:\n",
    "- Estructura de la red, capa por capa\n",
    "- Como van disminuyendo las dimensiones de cada capa, empezando por las dimensiones de la imágen (30,30) hasta la última capa = Número de clases (27)\n",
    "- Cantidad de parámetros que se van a calcular/entrenar: 80.635"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=keras.optimizers.RMSprop(learning_rate=learning_rate),\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "62/62 [==============================] - 8s 126ms/step - loss: 3.3256 - accuracy: 0.0469 - val_loss: 3.2973 - val_accuracy: 0.0455\n",
      "Epoch 2/50\n",
      "62/62 [==============================] - 3s 50ms/step - loss: 3.2780 - accuracy: 0.0408 - val_loss: 3.2878 - val_accuracy: 0.0398\n",
      "Epoch 3/50\n",
      "62/62 [==============================] - 3s 48ms/step - loss: 3.2839 - accuracy: 0.0388 - val_loss: 3.2720 - val_accuracy: 0.0625\n",
      "Epoch 4/50\n",
      "62/62 [==============================] - 3s 52ms/step - loss: 3.2529 - accuracy: 0.0612 - val_loss: 3.2673 - val_accuracy: 0.0682\n",
      "Epoch 5/50\n",
      "62/62 [==============================] - 3s 51ms/step - loss: 3.2249 - accuracy: 0.0939 - val_loss: 3.2489 - val_accuracy: 0.0852\n",
      "Epoch 6/50\n",
      "62/62 [==============================] - 3s 49ms/step - loss: 3.2271 - accuracy: 0.0714 - val_loss: 3.2285 - val_accuracy: 0.0795\n",
      "Epoch 7/50\n",
      "62/62 [==============================] - 3s 49ms/step - loss: 3.2028 - accuracy: 0.0837 - val_loss: 3.2106 - val_accuracy: 0.0795\n",
      "Epoch 8/50\n",
      "62/62 [==============================] - 3s 48ms/step - loss: 3.1698 - accuracy: 0.0939 - val_loss: 3.2010 - val_accuracy: 0.0909\n",
      "Epoch 9/50\n",
      "62/62 [==============================] - 3s 49ms/step - loss: 3.1538 - accuracy: 0.1082 - val_loss: 3.1798 - val_accuracy: 0.1250\n",
      "Epoch 10/50\n",
      "62/62 [==============================] - 3s 50ms/step - loss: 3.1746 - accuracy: 0.0755 - val_loss: 3.1473 - val_accuracy: 0.1250\n",
      "Epoch 11/50\n",
      "62/62 [==============================] - 3s 50ms/step - loss: 3.1123 - accuracy: 0.1020 - val_loss: 3.1486 - val_accuracy: 0.1136\n",
      "Epoch 12/50\n",
      "62/62 [==============================] - 3s 49ms/step - loss: 3.1196 - accuracy: 0.1327 - val_loss: 3.1241 - val_accuracy: 0.1136\n",
      "Epoch 13/50\n",
      "62/62 [==============================] - 3s 49ms/step - loss: 3.0748 - accuracy: 0.1082 - val_loss: 3.1144 - val_accuracy: 0.1193\n",
      "Epoch 14/50\n",
      "62/62 [==============================] - 3s 49ms/step - loss: 3.1106 - accuracy: 0.1184 - val_loss: 3.0910 - val_accuracy: 0.1307\n",
      "Epoch 15/50\n",
      "62/62 [==============================] - 3s 49ms/step - loss: 3.0533 - accuracy: 0.1367 - val_loss: 3.0691 - val_accuracy: 0.1136\n",
      "Epoch 16/50\n",
      "62/62 [==============================] - 3s 49ms/step - loss: 3.0489 - accuracy: 0.1102 - val_loss: 3.0733 - val_accuracy: 0.1364\n",
      "Epoch 17/50\n",
      "10/62 [===>..........................] - ETA: 1s - loss: 3.0001 - accuracy: 0.1875"
     ]
    }
   ],
   "source": [
    "history = model.fit(train_generator, \n",
    "                    steps_per_epoch = int(train_samples/batch_size), \n",
    "                    epochs = epochs, \n",
    "                    validation_data = validation_generator,\n",
    "                    validation_steps = int(validation_samples/batch_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.xlabel('# Epoca')\n",
    "plt.ylabel(\"Función de Pérdida (loss)\")\n",
    "plt.plot(history.history['loss'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En la gráfica de **loss** podemos ver cómo va disminuyendo nuestro *categorical_crossentropy* en cada epoca.  \n",
    "También podemos ver los valores finales del loss y el accuracy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate_generator(validation_generator, validation_samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una vez aprobemos nuestro modelo, podemos usarlo para predecir sobre imágenes nuevas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_path  = \"https://i.pinimg.com/originals/07/0d/3e/070d3e118fa9bcf14bf6a1004308c91b.jpg\"\n",
    "response = requests.get(img_path)\n",
    "img = Image.open(BytesIO(response.content))\n",
    "plt.imshow(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = io.imread(img_path)\n",
    "img = cv2.resize(img, (img_width, img_height)).astype(np.float32)\n",
    "img = np.expand_dims(img, axis=0)\n",
    "prediction = model.predict(img)\n",
    "prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El resultado de la predicción es la salida de la última neurona, es decir 27 valores que siginifican la probabilidad de que la imagen dada pertenesca a cada una de las 27 clases objetivo.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(prediction[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para saber, entonces, la clase que indica la predicción basta con elegir la probabilidad más alta del vector entregado, pues corresponden, por índice a las clases objetivo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes[prediction.argmax()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Taller:**  \n",
    "A pesar de que para este ejemplo, el modelo hizo una predicción correcta, el loss aún está un poco alto, y el accuracy esta muy bajo.  Además, el valor de *accuracy* para el set de entrenamiento fue más alto que el de pruebas, quiere decir que el modelo está aprendiendo de memoria casos particulres, en lugar de patrones generales (Overfit).  Para ello:\n",
    "- Juegue un poco con los hiperparámetros, capas y funciones de activación para mejorar estos valores.  Para ello peude usar la documentación de [keras](https://keras.io/api/)\n",
    "- Busque otra imagen (*.jpg*) de alguno de los logos en nuestro modelo y haga la predicción.  Funciona? Estamos safisfechos con este modelo? Podria mejorar?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5b. Transfer Learning  \n",
    "Para no tener que diseñar la red desde ceros, pensando qué capas, neuronas y funciones de activación usar, usamos Transfer Learning, en el que tomamos la estructura de una red ya contruida y especializada en una tarea específica (Ej: Reconocimiento de imagen).  \n",
    "Los siguientes pasos iniciales siguen siendo necesarios: \n",
    "- Definición de hiperprámetros\n",
    "- Generador de imagenes nuevas (Data Augemtnation)\n",
    "- Configuración de generadores para entrenamiento y pruebas  \n",
    "\n",
    "Pero en lugar de crear un modelo desde ceros, vamos a tomar un modelo ya existente, por ejemplo VGG16 y vamos a indicar que sus capas NO SON reentrenables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = keras.applications.VGG16(weights='imagenet', include_top=False)\n",
    "\n",
    "for layer in base_model.layers :\n",
    "    layer.trainable = False\n",
    "\n",
    "base_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vemos la estructura tan compleja que tiene esta red ya preentrenada, más de 14.5 millones de parámetros!  En este caso, también hemos indicado que este modelo no sea reentrenable.  \n",
    "También hemos indicado al modelo no incluir la \"cabeza\" de la red, pues las últimas capas las vamos a reescribir nosotros mismos:\n",
    "> Ya hemos visto una forma de definir la red: deinifiendo el modelo Sequencil vacío e ir añadiendo capas (*model.add*).  \n",
    "> Otra forma es definiendo cada capa, y agregándola como entrada de la capa siguiente:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input = keras.Input(shape=(img_width, img_height, 3),name = 'image_input')\n",
    "\n",
    "model_head = base_model(input)\n",
    "model_head = keras.layers.GlobalAveragePooling2D()(model_head)\n",
    "model_head = keras.layers.Dense(64)(model_head)\n",
    "model_head = keras.layers.Activation(activation='relu')(model_head)\n",
    "model_head = keras.layers.Dropout(0.4)(model_head)\n",
    "model_head = keras.layers.Dense(27)(model_head)\n",
    "model_head = keras.layers.Activation(activation='softmax')(model_head)\n",
    "\n",
    "my_model = keras.Model(input=input, output=model_head)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vemos ahora que, la estructura de nuestra nueva red es mucho más compleja, incluye los 14.5 millones de parámetros del modelo VGG16, adicionalmente hemos configurado la capa de entrada para que reciba las imágenes que tenemos preparadas, pero lo más importante, hemos configurado algunas capas adicionales como la **nueva cabeza** de la red.  \n",
    "Al final del resumen vemos que, los 14.5 millones de parámetros del modelo base (VGG16) no son entrenables, pero los 34mil de nuestra nueva cabeza sí, y estos son los valores que va a buscar el modelo.  \n",
    "\n",
    "Hemos creado en pocas líneas un modelo más complejo, pero más sencillo de entrenar.  Desde aquí la compilación y entrenamiento del modelo es igual a como ya lo hemos trabajado:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=keras.optimizers.rmsprop(learning_rate=learning_rate),\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como paso adicional de este laboratorio vamos a trabajar con los Callbacks que vimos la sesión pasada.  Son funciones que se ejecutan al final de cada época y que pueden ayudar a mejorar el rendimiento del modelo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mis_callbacks = [\n",
    "    # Modificar el LR si el modelo no mejora:\n",
    "    keras.callbacks.ReduceLROnPlateau(monitor = \"val_loss\", factor=.1, patience=5),\n",
    "    # Configurar que el modelo pare si no mejora:\n",
    "    keras.callbacks.EarlyStopping(monitor='val_accuracy', patience=10, mode='auto'),\n",
    "    # Ir guardando el mejor modelo\n",
    "    keras.callbacks.ModelCheckpoint(\"models/logos_checkpoints.h5\", monitor='val_loss', save_best_only=True)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = my_model.fit(train_generator, \n",
    "                    steps_per_epoch = int(train_samples/batch_size), \n",
    "                    epochs = epochs, \n",
    "                    validation_data = validation_generator,\n",
    "                    validation_steps = int(validation_samples/batch_size),\n",
    "                    callbacks = mis_callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.xlabel('# Epoca')\n",
    "plt.ylabel(\"Función de Pérdida (loss)\")\n",
    "plt.plot(history.history['loss'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.xlabel('# Epoca')\n",
    "plt.ylabel(\"Métrica (Accuracy)\")\n",
    "plt.plot(history.history['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_model.evaluate_generator(validation_generator, validation_samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En comparación con nuestro modelo inicial (*Loss = 2.46, Accuracy=0.26*), este modelo da mejores resultados!  \n",
    "Vamos ahora a probarlo con imágenes nuevas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_path  = \"https://i.pinimg.com/originals/02/ac/cd/02accd95989df4cde2f57adcd508dbcd.jpg\"\n",
    "response = requests.get(img_path)\n",
    "img = Image.open(BytesIO(response.content))\n",
    "plt.imshow(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = io.imread(img_path)\n",
    "img = cv2.resize(img, (img_width, img_height)).astype(np.float32)\n",
    "img = np.expand_dims(img, axis=0)\n",
    "prediction = my_model.predict(img)\n",
    "prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(prediction[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes[prediction.argmax()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Taller**  \n",
    "**Ups!** Al modelo le falta un poco de afinamiento, intente:\n",
    "- Cambiar el modelo base VGG16 por otros modelos como *VGG19, ResNet50, InceptionV3, MobileNet, Xception*.  Para ello peude usar la documentación de [keras](https://keras.io/api/)\n",
    "- Juegue un poco con los hiperparámetros, capas y funciones de activación en la **cabeza de la red** para mejorar los valores de loss, accuracy y las predicciones.\n",
    "- Busque otra imagen (*.jpg*) de alguno de los logos en nuestro modelo y haga la predicción.  Funciona? Estamos safisfechos con este modelo? Podria mejorar?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
