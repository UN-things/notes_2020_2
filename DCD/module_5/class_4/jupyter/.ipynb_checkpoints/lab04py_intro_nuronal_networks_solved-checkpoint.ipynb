{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab04 - Introducción a Redes Neuronales  \n",
    "\n",
    "En la sesión de hoy ya aprendimos qué es una **Red Neuronal** (*Realmente es una ecuación matemática*) y cómo **\"aprende\"** (*Minizar funciones matemáticas*).  También aprendimos algunos conceptos e hiperparámetros como:\n",
    "- Perceptrón\n",
    "- Neuronas\n",
    "- Capas Ocultas\n",
    "- Función de Pérdida (Loss)\n",
    "- Función de Activación\n",
    "- Descenso de Gradiente\n",
    "- Retropropagación\n",
    "- Batch\n",
    "- Epoca\n",
    "- Learning Rate (Taza de Aprendizaje)\n",
    "\n",
    "Es mucha información!  Ahora vamos a aplicarlo todo!\n",
    "\n",
    "## 4a. Tensorflow Playground\n",
    "\n",
    "Tensorflow es una librería desarrollada por [Google](https://www.tensorflow.org/?hl=es-419) para aprendizaje profundo. Permite configurar de forma fácil todos los conceptos de redes neuronales como capas ocultas, funciones de activación, y otro hiperarámetros; además, puede usarse tanto en R como en Python!  \n",
    "\n",
    "Una de las ventajas de Tensorflow, es que su filosofía es siempre acercar la Inteligencia Artificial a las personas de forma amigable y estructurada sin necesidad de conocer todos los cálculos y matemáticas que suceden por debajo (Aunque siempre es mejor conocerlos!).  Como parte de esta metodología, se creó [Tensorflow Playground](https://playground.tensorflow.org/), una plataforma gráfica que permite visualizar la implementación y entrenamiento de una Red Neuronal, y los efectos de configurar cada uno de sus hiperparámetros!  \n",
    "\n",
    "### Taller:  \n",
    "Dentro de la plataforma de **TensorFlow Playground** vamos a tratar de construir una red neuronal para cada uno de los siguientes sets de datos.  Trate de construirla de manera que, graficamente divida a los datos por color en la menor cantidad de tiempo (*Epocas*) posible con el *Loss* más bajo posible.  \n",
    "Algunos de los hiperparámetros con los que puede *jugar* son: \n",
    "- Cantidad de capas ocultas\n",
    "- Cantidad de neuronas en cada capa (Excepto la capa final: 2)\n",
    "- Taza de Aprendizaje (Learning Rate)\n",
    "- Función de Activación iniciales y final\n",
    "- Regularización (Y su respectivo Lambda)\n",
    "- % de test de entrenamiento y pruebas\n",
    "- Batch Size\n",
    "- Epocas (Tú decides cuándo parar!)\n",
    "\n",
    "\n",
    "[**1. Regresión**  ](https://playground.tensorflow.org/#activation=tanh&batchSize=4&dataset=circle&regDataset=reg-gauss&learningRate=1&regularizationRate=0&noise=0&networkShape=4,4,2&seed=0.48832&showTestData=false&discretize=false&percTrainData=80&x=true&y=true&xTimesY=true&xSquared=true&ySquared=true&cosX=false&sinX=true&cosY=false&sinY=true&collectStats=false&problem=regression&initZero=false&hideText=false)  \n",
    "\n",
    "[**2. Clasificación**](https://playground.tensorflow.org/#activation=tanh&batchSize=10&dataset=spiral&regDataset=reg-plane&learningRate=1&regularizationRate=0&noise=0&networkShape=4,2&seed=0.88013&showTestData=false&discretize=false&percTrainData=50&x=true&y=true&xTimesY=false&xSquared=false&ySquared=false&cosX=false&sinX=false&cosY=false&sinY=false&collectStats=false&problem=classification&initZero=false&hideText=false)  \n",
    "\n",
    "## 4b. Mi Primera Red Neuronal  \n",
    "\n",
    "Una de las librerías más cómodas y amigable para definir redes neuronales es [Keras](https://keras.io/api/).  Es amigable porque empaqueta en una sola librería todas las librerías necesarias para realizar las operaciones tan complejas que comprenden una Red Neuronal y simplifica su sintaxis.  \n",
    "\n",
    "Sin embargo, aunque usarla sea muy fácil, en ocasiones es difícil su instalación por toda la complejidad que lleva por dentro:  \n",
    "\n",
    "Trate de instalar la librería *keras* y de importarla ejecutando la siguiente celda.  Si no es posible de forma fácil vamos a trabajar en la plataforma [Google Colab](https://colab.research.google.com/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3/dist-packages/requests/__init__.py:91: RequestsDependencyWarning: urllib3 (1.26.2) or chardet (3.0.4) doesn't match a supported version!\n",
      "  RequestsDependencyWarning)\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recordemos que una Red Neuronal es, al fin y al cabo, una ecuación.  Vamos a entrenar una Red Neuronal/Ecuación sencilla, de la cuál ya conocemos los coeficientes para poder validar su funcionamiento ya que estamos aprendiendo, por ejemplo: **Transformar Grados Centígrados a Grados Farenheit**.  \n",
    "Obviamente, podemos recordar o [buscar](https://www.google.com/search?sxsrf=ALeKk02bpoMAGFjDz3hSNDjPDDcynzjCTg%3A1602822358031&source=hp&ei=1SCJX-uDO-fO5gLo-InoCA&q=c+to+f&oq=c+to+f&gs_lcp=CgZwc3ktYWIQAzIECCMQJzIECAAQQzIFCAAQywEyBQgAEMsBMgUIABDLATIFCAAQywEyBQgAEMsBMgUIABDLATIFCAAQywEyBQgAEMsBOgcIABCxAxBDOgUIABCxAzoCCAA6CAgAELEDEIMBOgUILhCxAzoHCAAQFBCHAjoCCC5Q9w5Y4BRg0xVoAHAAeACAAY0BiAGFBZIBAzIuNJgBAKABAaoBB2d3cy13aXo&sclient=psy-ab&ved=0ahUKEwirwqPKorjsAhVnp1kKHWh8Ao0Q4dUDCAc&uact=5) la ecuación y crear una función tradicional que haga el cálculo:  \n",
    "$$ F = 2/9*C + 32 $$\n",
    "\n",
    "... pero esto no sería Machine Learning!  Además, no siempre vamos a conocer la ecuación de antemano.  \n",
    "\n",
    "Por el contrario, vamos a crear un modelo basado en *algunos* valores de grados Celsius, para los que conocemos sus correspondientes valores Farenheit y vamos a dejar que el modelo encuentre la relación entre ellos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = [-40, -18, 0, 10, 20, 30, 40,  50, 60, 70, 80, 90]\n",
    "f = [-40,  0, 32, 50, 68, 86, 104, 122, 140, 158, 176, 194]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recordemos los tres pasos para implementar una Red Neuronal que vimos en clase:  \n",
    "\n",
    "**1. Definir el Modelo**  \n",
    "El modelo se define como un listado de capas a las cuales debemos configurar para cada una, el número de neuronas y la función de activación.  Por ahora, para empezar, vamos a definir solo una capa, Densa, con una sola Neurona:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 1)                 2         \n",
      "=================================================================\n",
      "Total params: 2\n",
      "Trainable params: 2\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = keras.Sequential()\n",
    "model.add(keras.layers.Dense(1, input_shape=[1]))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El resumen del modelo nos muestra que, aunque no ha sido entrenada, la red tiene una sola capa (*dense_x*), con una sola salida, y que está preparado para entrenar dos parámetros: La neurona de la primera capa y el bias.  \n",
    "\n",
    "**2. Compilar el Modelo**  \n",
    "Ahora debemos decirle al Modelo, cómo medir si se está equivocando (*Función de pérdida*), cómo evaluarse a sí mismo (*Métricas*) y cómo ajustar sus valores (*Optimizador*) para ir mejorando en cada época:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='mean_squared_error',\n",
    "              optimizer=keras.optimizers.Adam(learning_rate=0.1),\n",
    "              metrics=['mean_absolute_error'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3. Entrenar el Modelo**  \n",
    "Listo! Ya le indicamos a la red su estructura, y sus métodos de aprendizaje, ahora solo falta indicarle con qué datos se va a entrenar y por cuanto tiempo lo va a hacer (épocas):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 4656.6572 - mean_absolute_error: 60.9285\n",
      "Epoch 2/500\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 1172.7766 - mean_absolute_error: 33.0298\n",
      "Epoch 3/500\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 540.4137 - mean_absolute_error: 18.7985\n",
      "Epoch 4/500\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 551.4021 - mean_absolute_error: 18.9227\n",
      "Epoch 5/500\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 456.0021 - mean_absolute_error: 17.2890\n",
      "Epoch 6/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 449.5289 - mean_absolute_error: 17.6312\n",
      "Epoch 7/500\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 424.5962 - mean_absolute_error: 16.6549\n",
      "Epoch 8/500\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 401.4955 - mean_absolute_error: 15.8417\n",
      "Epoch 9/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 506.2490 - mean_absolute_error: 18.0473\n",
      "Epoch 10/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 380.0209 - mean_absolute_error: 15.5905\n",
      "Epoch 11/500\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 340.6613 - mean_absolute_error: 14.6540\n",
      "Epoch 12/500\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 352.4669 - mean_absolute_error: 15.4078\n",
      "Epoch 13/500\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 312.0580 - mean_absolute_error: 13.6793\n",
      "Epoch 14/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 311.0025 - mean_absolute_error: 14.3357\n",
      "Epoch 15/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 310.4092 - mean_absolute_error: 14.8755\n",
      "Epoch 16/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 261.8620 - mean_absolute_error: 12.7784\n",
      "Epoch 17/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 257.2236 - mean_absolute_error: 12.2962\n",
      "Epoch 18/500\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 238.7547 - mean_absolute_error: 12.4804\n",
      "Epoch 19/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 246.6012 - mean_absolute_error: 13.5126\n",
      "Epoch 20/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 214.6317 - mean_absolute_error: 11.3191\n",
      "Epoch 21/500\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 208.0338 - mean_absolute_error: 11.4198\n",
      "Epoch 22/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 181.7466 - mean_absolute_error: 10.5380\n",
      "Epoch 23/500\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 210.7352 - mean_absolute_error: 11.9553\n",
      "Epoch 24/500\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 170.7704 - mean_absolute_error: 10.7908\n",
      "Epoch 25/500\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 159.0998 - mean_absolute_error: 10.2577\n",
      "Epoch 26/500\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 153.0066 - mean_absolute_error: 10.1029\n",
      "Epoch 27/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 143.0544 - mean_absolute_error: 9.6915\n",
      "Epoch 28/500\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 121.9387 - mean_absolute_error: 8.7501\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(c, f, epochs=500, steps_per_epoch=13)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ".\n",
    "\n",
    "Estas 500 épocas se ejecutaron muy rápido!  Cuando estemos trabajando con un dataset real/más grande (Con más atributos y muchísimos más registros) es posible que tome mucho más tiempo, por esto es importante tener en cuenta la cantidad de épocas y pasos en cada época.  \n",
    "\n",
    "En este ejemplo, vemos como, época a época, el modelo va evolucionando: Disminuye el valor de pérdida (*loss*) y aumentan/disminuyen las métricas elegidas (En este caso, *Error Medio Absoluto*, disminuye!).  \n",
    "Es mucho mejor ver esta evolución del modelo gráficamente para tomar decisiones:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.xlabel('# Epoca')\n",
    "plt.ylabel(\"Función de Pérdida (loss)\")\n",
    "plt.plot(history.history['loss'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aquí vemos de forma gráfica cómo, en su primer intento (epoca), el modelo se equivocó muchísimo \"adivinando\" los coeficientes de la ecuación, es decir el valor del *loss* dio muy alto al elegir los coeficientes aleatoriamente.  Sin embargo, en las siguientes épocas, estos valores se fueron optimizando, disminuyendo el *loss* a casi 0.  \n",
    "\n",
    "**Ejercicio:**  \n",
    "Compile y entrene nuevamente el modelo (*model.compile()* y *model.fit()*) sin cambiar ningún parámetro.  Genere nuevamente la gráfica de pérdida.  Qué nota?  \n",
    "\n",
    "Es muy posible que la gráfica sea diferente cada vez (Es decir: Las Redes Neuronales son **No Determinísticas**).  Esto se debe a:  \n",
    "- Los valores iniciales se eligen aleatoriamente y son distintos cada vez que se compila/entrena el modelo.  \n",
    "- Lo mismo pasa con cada época, para modificar los coeficientes de una época a otra usando la función de optimización se toman valores aleatorios muy pequeños (Taza de Aprendizaje, LR).  \n",
    "- En todo caso, aunque se trata de varias decisiones aleatorias, las \"reglas del juego\": optimizador, métricas y funcion depérdida, ayudan al modelo a encaminarse a una buena solución  \n",
    "\n",
    "La función model.evaluate() nos va a entregar las métricas correspondientes a la última epoca, la que definió el modelo finalmente:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for p in zip(['loss', 'mean_absolute_error'], model.evaluate(c, f, verbose=False)) :\n",
    "    print(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos a probar el Modelo!  Intentemos pasar un valor con el que no haya sido entrenado (En este caso, como se trata de una ecuación científica muy común, podemos saber el valor real que debería haber retornado)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model.predict([38])) # Debería dar 100\n",
    "print(model.predict([100])) # Debería dar 212\n",
    "print(model.predict([200])) # Debería dar 392"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como una prueba adicional, vamos a hacer algo que solo podemos hacer en este caso y es validar los pesos asignados por el modelo. Esto porque asignamos una sola capa de una sola neurona (Entonces es sencillo) y porque conocemos la ecuación que estamos buscando (Esto no ocurre nunca!):  \n",
    "$$ F = 2/9*C + 32 = 1.8*C + 32$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.get_weights()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Taller:**  \n",
    "\n",
    "Qué pasa si añadimos más capas y neuronas a la Red? O si cambiamos el método de aprendizaje?\n",
    "- En la definición del modelo, puede agregar cualquiera o varias de las siguientes capas a la función, en cualquier orden y modificar los valores *x* de sus hiperparámetros:\n",
    ">model = keras.Sequential()  \n",
    "    **model.add(keras.layers.Dense(x, input_shape=[1]))** # *No cambiar input_shape (#entradas este caso = 1)*\n",
    "    **model.add(keras.layers.Dense(x))** # *A partir de la 2 capa podemos \"jugar\" con la cantidad de neuronas*\n",
    "    **model.add(keras.layers.Dropout(x))** # *También podemos modificar otrss parámetros de cada una*  \n",
    "    model.add(keras.layers.Dense(1)) # *No cambiar (#salidas este caso = 1)*  \n",
    "- En las capas principales puede cambiar también la función de activación por los valores: *activation = \"relu\", \"softmax\", \"sigmoid\", \"tanh\"*\n",
    "- En la compilación del modelo, intente con otros optimizadores, por ejemplo: *SGD, Adadelta, Adam, RMSprop*\n",
    "- En el entrenamiento del modelo, pruebe modificando los valores de *epochs* y *steps_per_epoch*  \n",
    "> Tenga en cuenta que si va a trabajar un problema de **Clasificación**, también puede \"jugar\" con *loss = \"categorical_crossentropy\"* y *metrics = [\"accuracy\", \"recall\"]*...\n",
    "\n",
    "Analice cómo cambia la estructura de la red (*summary()*), y como evoluciona en entrenamiento de acuerdo a su nueva configuración (Mejora? Empeora? Aprende más rápido?)  \n",
    "\n",
    "También note cómo, si agregó más capas o neuronas a la red, la última función de este ejercicio (*get_weights()*) ahora arroja muchos más coeficientes de variables que realmente no podemos entender (**No explicativo**), añadiento muchísima más complejidad al modelo aunque también arroja un resulado **correcto**."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
