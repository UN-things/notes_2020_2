---
title: "Universidad Nacional de Colombia"
subtitle: "Diplomado Ciencia de datos"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Análisis de texto

En este caso práctico haremos una revisión general de cómo podemos analizar y procesar textos desde una perspectiva descriptiva.

El caso está estructurado así:

1. Carga y exploración de texto
2. Procesos de limpieza y transformación en texto
3. Representaciones de conteos

**Contexto:** Muchas empresas y entidades estan monitoreando constantemente los comentarios que hay en redes sociales sobre sus productos y servicios para identificar rápidamente molestias e inquietudes de sus clientes. Así poder tomar decisiones y medidas eficientemente evitando que su imagen se vea damnificada

**Pregunta de negocio:** En este caso analizaremos una muestra de tweets relacionados con el banco Davivienda. El objetivo es analizar cuáles pueden ser los temas tendencia en estos tweets y qué hallazgos se encuentran en ellos

```{r}
# ruta directorio
#setwd('C:/Users/Juliana/Desktop/Diplomado/D_2020/Casos/Casos/TextMining') 


# cargue librerias 
library(tokenizers)
library(tidyverse)
library(tm)
library(stopwords)
library(textstem)
library(wordcloud)
library(textstem)
library(udpipe)
library(dplyr)
```


```{r}
## Cargue de datos
tx <- read.csv('tweets.csv')

head(tx)
```


## Manejo de textos

**Tokenizar oraciones**

```{r}
# tokenizar oraciones
tokenize_sentences(as.character(tx[1,1])) 

```


**Tokenizar palabras**

```{r}
# tokenizar palabras
tokenize_words(as.character(tx[2,1]))

```

**Tamaño de tweets**

```{r}
# tamaño de los tweets
tx$largo = nchar(as.character(tx$texto))
mean(tx$largo)
min(tx$largo)
max(tx$largo)

hist(tx$largo)
```



## Palabras o tokens mas frecuentes

```{r}
## tokens mas frecuentes
palabras <- paste(tx$texto,collapse='\n')
words <- tokenize_words(palabras,lowercase = FALSE, strip_numeric = FALSE,strip_punct = FALSE)
asdf<-table(words)
tab <- tibble(word = names(asdf), count = as.numeric(asdf))
arrange(tab,desc(count))
```

Al sacar los tokens más comunes en esta muestra de tweets observamos como algunos signos de puntuación resultan ser los más comunes. Esto indica que debemos hacer un proceso de limpieza de los textos


## Limpieza y pre-procesamiento

**Llevar todas las palabras a minúsculas**

```{r}
# to lower 
as.character(tx[1,1])
tolower(as.character(tx[1,1]))
```


**Remover @, # con expresiones regulares**

```{r}
# caracteres especiales
gsub('[@w+|#]','',as.character(tx[1,1]))
```

**Remover signos de puntuación**

```{r}
# puntuación 
gsub('[[:punct:] ]+',' ',as.character(tx[1,1]))
removePunctuation(as.character(tx[1,1])) # librería tm
```

Luego de hacer esta limpieza inicial cómo varían las palabras/tokens más frecuente

```{r}

# frecuencias con modificaciones 
words <- tokenize_words(palabras,lowercase = TRUE, strip_numeric = FALSE,strip_punct = TRUE)
asdf<-table(words)
tab <- tibble(word = names(asdf), count = as.numeric(asdf))
arrange(tab,desc(count))
```


**¿Qué tipo de palabras encontramos en la lista de las más frecuentes?**

Naturalmente las palabras más comunes suelen ser "palabras vacías" que no añaden mucho significado al texto como artículos, pronombres y preposiciones. Lo más aconsejable es no considerarlas en el análisis ya que traen mucho ruido y no permiten identificar aquellas palabras que si son relevantes para el análisis

**stopwords**

```{r}

## stopwords
stopwords::stopwords("es")
```


Removamos de nuestro texto las stopwords. Una vez hecha la limpieza ya podemos entonces verificar cuáles son las palabras más frecuentes en esta muestra de tweets

```{r}
# con tm 
tx$texto_l <- removeWords(as.character(tx$texto),stopwords("spanish"))
palabras <- paste(tx$texto_l,collapse='\n')
words <- tokenize_words(palabras,lowercase = TRUE, strip_numeric = FALSE,strip_punct = TRUE)
asdf<-table(words)
tab <- tibble(word = names(asdf), count = as.numeric(asdf))
arrange(tab,desc(count))
```

```{r}
# con la funcion tokenize_words
palabras <- paste(tx$texto,collapse='\n')
words <- tokenize_words(palabras,lowercase = TRUE, strip_numeric = FALSE,strip_punct = TRUE,stopwords = stopwords::stopwords("es"))
asdf<-table(words)
tab <- tibble(word = names(asdf), count = as.numeric(asdf))
arrange(tab,desc(count))

```

**n-gramas**

```{r}
## ngrams
# bigram
test <- tokenize_ngrams(palabras,n=2,lowercase = TRUE,stopwords = stopwords::stopwords("es"))
pre <- table(test)
tab <- tibble(ngram = names(pre), count = as.numeric(pre))
arrange(tab,desc(count))

```

```{r}
# trigram
test <- tokenize_ngrams(palabras,n=3,lowercase = TRUE,stopwords = stopwords::stopwords("es"))
pre <- table(test)
tab <- tibble(ngram = names(pre), count = as.numeric(pre))
arrange(tab,desc(count))
```


## Llevar texto a vectores

```{r}
#  matriz tdm
corp <- Corpus(VectorSource(as.character(tx$texto)))
tdm <- as.matrix(DocumentTermMatrix(corp,control = list(removePunctuation = TRUE,stopwords = TRUE)))
tdm[1:10,1:15]
```

**Wordcloud

```{r}
# wordcloud
corp <- corp %>%
  tm_map(removeNumbers) %>%
  tm_map(removePunctuation) %>%
  tm_map(stripWhitespace)
corp <- tm_map(corp, content_transformer(tolower))
corp <- tm_map(corp, removeWords, stopwords("spanish"))


dtm <- TermDocumentMatrix(corp) 
matrix <- as.matrix(dtm) 
words <- sort(rowSums(matrix),decreasing=TRUE) 
df <- data.frame(word = names(words),freq=words)

set.seed(1234) # for reproducibility 
wordcloud(words = df$word, freq = df$freq, min.freq = 5,
          max.words=200, random.order=FALSE, rot.per=0.35,
          colors=brewer.pal(8, "Dark2"))
```


## Otras transformaciones de texto 



**Lemmatización**


```{r}
vector <- c("trouble", "troubling", "troubled")
lemmatize_words(vector)
vector <- c("corria", "corre", "corren")
lemmatize_words(vector)
```


**POS**
```{r}
## english 
sent <- c("Juliana is having a good day")
df <- data.frame(sent, stringAsFactors = FALSE)
udmodel <- udpipe_download_model(language = "english")
udmodel <- udpipe_load_model(file = udmodel$file_model)

x <- udpipe_annotate(udmodel,df$sent)
x <- as.data.frame(x)
x %>% select(token, upos)
```


```{r}
# spanish
sent <- c("Juliana tiene un buen día")
df <- data.frame(sent, stringAsFactors = FALSE)


udmodel <- udpipe_download_model(language = "spanish")
udmodel <- udpipe_load_model(file = udmodel$file_model)


x <- udpipe_annotate(udmodel,df$sent)
x <- as.data.frame(x)
x %>% select(token, upos)
```










